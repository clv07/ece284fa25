{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7ZjSqWYkBoC",
        "outputId": "e159ed7a-32ec-478a-8c24-b89b4d701b03"
      },
      "id": "P7ZjSqWYkBoC",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "radical-fifty",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "radical-fifty",
        "outputId": "f272478b-7599-4a87-9bbe-0b538f9b0f53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Building model...\n",
            "ResNet_Cifar(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): QuantConv2d(\n",
            "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_quant): weight_quantize_fn()\n",
            "      )\n",
            "      (conv2): QuantConv2d(\n",
            "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_quant): weight_quantize_fn()\n",
            "      )\n",
            "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): QuantConv2d(\n",
            "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_quant): weight_quantize_fn()\n",
            "      )\n",
            "      (conv2): QuantConv2d(\n",
            "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_quant): weight_quantize_fn()\n",
            "      )\n",
            "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): QuantConv2d(\n",
            "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_quant): weight_quantize_fn()\n",
            "      )\n",
            "      (conv2): QuantConv2d(\n",
            "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_quant): weight_quantize_fn()\n",
            "      )\n",
            "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): QuantConv2d(\n",
            "        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "        (weight_quant): weight_quantize_fn()\n",
            "      )\n",
            "      (conv2): QuantConv2d(\n",
            "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_quant): weight_quantize_fn()\n",
            "      )\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): QuantConv2d(\n",
            "          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (weight_quant): weight_quantize_fn()\n",
            "        )\n",
            "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): QuantConv2d(\n",
            "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_quant): weight_quantize_fn()\n",
            "      )\n",
            "      (conv2): QuantConv2d(\n",
            "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_quant): weight_quantize_fn()\n",
            "      )\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): QuantConv2d(\n",
            "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_quant): weight_quantize_fn()\n",
            "      )\n",
            "      (conv2): QuantConv2d(\n",
            "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_quant): weight_quantize_fn()\n",
            "      )\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): QuantConv2d(\n",
            "        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "        (weight_quant): weight_quantize_fn()\n",
            "      )\n",
            "      (conv2): QuantConv2d(\n",
            "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_quant): weight_quantize_fn()\n",
            "      )\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): QuantConv2d(\n",
            "          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (weight_quant): weight_quantize_fn()\n",
            "        )\n",
            "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): QuantConv2d(\n",
            "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_quant): weight_quantize_fn()\n",
            "      )\n",
            "      (conv2): QuantConv2d(\n",
            "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_quant): weight_quantize_fn()\n",
            "      )\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): QuantConv2d(\n",
            "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_quant): weight_quantize_fn()\n",
            "      )\n",
            "      (conv2): QuantConv2d(\n",
            "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_quant): weight_quantize_fn()\n",
            "      )\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
            "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:14<00:00, 11.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/')\n",
        "from models import *\n",
        "\n",
        "global best_prec\n",
        "use_gpu = torch.cuda.is_available()\n",
        "print('=> Building model...')\n",
        "\n",
        "batch_size = 128\n",
        "model_name = \"ResNet20_quant\"\n",
        "model = resnet20_quant()\n",
        "\n",
        "print(model)\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
        "\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ]))\n",
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ]))\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "print_freq = 100 # every 100 batches, accuracy printed. Here, each batch includes \"batch_size\" data points\n",
        "# CIFAR10 has 50,000 training data, and 10,000 validation data.\n",
        "\n",
        "def train(trainloader, model, criterion, optimizer, epoch):\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (input, target) in enumerate(trainloader):\n",
        "        # measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        input, target = input.cuda(), target.cuda()\n",
        "\n",
        "        # compute output\n",
        "        output = model(input)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        prec = accuracy(output, target)[0]\n",
        "        losses.update(loss.item(), input.size(0))\n",
        "        top1.update(prec.item(), input.size(0))\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "\n",
        "        if i % print_freq == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
        "                   epoch, i, len(trainloader), batch_time=batch_time,\n",
        "                   data_time=data_time, loss=losses, top1=top1))\n",
        "\n",
        "\n",
        "\n",
        "def validate(val_loader, model, criterion ):\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    with torch.no_grad():\n",
        "        for i, (input, target) in enumerate(val_loader):\n",
        "\n",
        "            input, target = input.cuda(), target.cuda()\n",
        "\n",
        "            # compute output\n",
        "            output = model(input)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            prec = accuracy(output, target)[0]\n",
        "            losses.update(loss.item(), input.size(0))\n",
        "            top1.update(prec.item(), input.size(0))\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            if i % print_freq == 0:  # This line shows how frequently print out the status. e.g., i%5 => every 5 batch, prints out\n",
        "                print('Test: [{0}/{1}]\\t'\n",
        "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
        "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
        "                   top1=top1))\n",
        "\n",
        "    print(' * Prec {top1.avg:.3f}% '.format(top1=top1))\n",
        "    return top1.avg\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def save_checkpoint(state, is_best, fdir):\n",
        "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
        "    torch.save(state, filepath)\n",
        "    if is_best:\n",
        "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    \"\"\"For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs\"\"\"\n",
        "    adjust_list = [150, 225]\n",
        "    if epoch in adjust_list:\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = param_group['lr'] * 0.1\n",
        "\n",
        "#model = nn.DataParallel(model).cuda()\n",
        "#all_params = checkpoint['state_dict']\n",
        "#model.load_state_dict(all_params, strict=False)\n",
        "#criterion = nn.CrossEntropyLoss().cuda()\n",
        "#validate(testloader, model, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "junior-reminder",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "junior-reminder",
        "outputId": "44c676b3-9947-4660-d6e7-daf87ef518c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [0][0/391]\tTime 0.240 (0.240)\tData 0.141 (0.141)\tLoss 0.2065 (0.2065)\tPrec 92.969% (92.969%)\n",
            "Epoch: [0][100/391]\tTime 0.061 (0.083)\tData 0.002 (0.005)\tLoss 0.1570 (0.2015)\tPrec 95.312% (92.659%)\n",
            "Epoch: [0][200/391]\tTime 0.063 (0.076)\tData 0.003 (0.004)\tLoss 0.1752 (0.2031)\tPrec 96.094% (92.907%)\n",
            "Epoch: [0][300/391]\tTime 0.063 (0.078)\tData 0.002 (0.004)\tLoss 0.1741 (0.2021)\tPrec 93.750% (92.948%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.160 (0.160)\tLoss 0.3190 (0.3190)\tPrec 88.281% (88.281%)\n",
            " * Prec 87.710% \n",
            "best acc: 87.710000\n",
            "Epoch: [1][0/391]\tTime 0.381 (0.381)\tData 0.231 (0.231)\tLoss 0.2676 (0.2676)\tPrec 90.625% (90.625%)\n",
            "Epoch: [1][100/391]\tTime 0.060 (0.077)\tData 0.002 (0.006)\tLoss 0.1542 (0.1939)\tPrec 95.312% (93.472%)\n",
            "Epoch: [1][200/391]\tTime 0.153 (0.080)\tData 0.006 (0.005)\tLoss 0.2264 (0.1916)\tPrec 93.750% (93.443%)\n",
            "Epoch: [1][300/391]\tTime 0.063 (0.076)\tData 0.008 (0.004)\tLoss 0.2545 (0.1965)\tPrec 90.625% (93.202%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.163 (0.163)\tLoss 0.2807 (0.2807)\tPrec 89.844% (89.844%)\n",
            " * Prec 87.930% \n",
            "best acc: 87.930000\n",
            "Epoch: [2][0/391]\tTime 0.267 (0.267)\tData 0.144 (0.144)\tLoss 0.2471 (0.2471)\tPrec 91.406% (91.406%)\n",
            "Epoch: [2][100/391]\tTime 0.071 (0.073)\tData 0.002 (0.004)\tLoss 0.1897 (0.2036)\tPrec 94.531% (92.884%)\n",
            "Epoch: [2][200/391]\tTime 0.063 (0.076)\tData 0.002 (0.004)\tLoss 0.1758 (0.2019)\tPrec 95.312% (92.910%)\n",
            "Epoch: [2][300/391]\tTime 0.082 (0.076)\tData 0.008 (0.004)\tLoss 0.1574 (0.2009)\tPrec 94.531% (92.927%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.157 (0.157)\tLoss 0.2354 (0.2354)\tPrec 93.750% (93.750%)\n",
            " * Prec 88.180% \n",
            "best acc: 88.180000\n",
            "Epoch: [3][0/391]\tTime 0.233 (0.233)\tData 0.142 (0.142)\tLoss 0.1642 (0.1642)\tPrec 94.531% (94.531%)\n",
            "Epoch: [3][100/391]\tTime 0.062 (0.082)\tData 0.002 (0.005)\tLoss 0.2277 (0.1943)\tPrec 92.188% (93.147%)\n",
            "Epoch: [3][200/391]\tTime 0.059 (0.076)\tData 0.002 (0.004)\tLoss 0.2505 (0.1920)\tPrec 92.188% (93.361%)\n",
            "Epoch: [3][300/391]\tTime 0.083 (0.077)\tData 0.004 (0.004)\tLoss 0.2674 (0.1959)\tPrec 95.312% (93.249%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.247 (0.247)\tLoss 0.2367 (0.2367)\tPrec 91.406% (91.406%)\n",
            " * Prec 88.010% \n",
            "best acc: 88.180000\n",
            "Epoch: [4][0/391]\tTime 0.245 (0.245)\tData 0.138 (0.138)\tLoss 0.1426 (0.1426)\tPrec 94.531% (94.531%)\n",
            "Epoch: [4][100/391]\tTime 0.061 (0.070)\tData 0.002 (0.004)\tLoss 0.1462 (0.1893)\tPrec 96.094% (93.410%)\n",
            "Epoch: [4][200/391]\tTime 0.068 (0.075)\tData 0.002 (0.004)\tLoss 0.1612 (0.1939)\tPrec 96.875% (93.194%)\n",
            "Epoch: [4][300/391]\tTime 0.092 (0.072)\tData 0.003 (0.003)\tLoss 0.1320 (0.1922)\tPrec 94.531% (93.301%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.152 (0.152)\tLoss 0.2262 (0.2262)\tPrec 92.969% (92.969%)\n",
            " * Prec 87.860% \n",
            "best acc: 88.180000\n",
            "Epoch: [5][0/391]\tTime 0.237 (0.237)\tData 0.134 (0.134)\tLoss 0.2225 (0.2225)\tPrec 93.750% (93.750%)\n",
            "Epoch: [5][100/391]\tTime 0.116 (0.077)\tData 0.009 (0.005)\tLoss 0.2174 (0.1814)\tPrec 92.188% (93.812%)\n",
            "Epoch: [5][200/391]\tTime 0.088 (0.075)\tData 0.004 (0.004)\tLoss 0.2075 (0.1888)\tPrec 92.969% (93.490%)\n",
            "Epoch: [5][300/391]\tTime 0.120 (0.076)\tData 0.004 (0.004)\tLoss 0.2289 (0.1899)\tPrec 92.969% (93.418%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.161 (0.161)\tLoss 0.2434 (0.2434)\tPrec 91.406% (91.406%)\n",
            " * Prec 87.940% \n",
            "best acc: 88.180000\n",
            "Epoch: [6][0/391]\tTime 0.256 (0.256)\tData 0.136 (0.136)\tLoss 0.2036 (0.2036)\tPrec 93.750% (93.750%)\n",
            "Epoch: [6][100/391]\tTime 0.061 (0.082)\tData 0.004 (0.005)\tLoss 0.1519 (0.1839)\tPrec 96.094% (93.533%)\n",
            "Epoch: [6][200/391]\tTime 0.134 (0.075)\tData 0.004 (0.004)\tLoss 0.2066 (0.1875)\tPrec 92.188% (93.424%)\n",
            "Epoch: [6][300/391]\tTime 0.067 (0.076)\tData 0.003 (0.004)\tLoss 0.1817 (0.1881)\tPrec 92.188% (93.366%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.202 (0.202)\tLoss 0.3512 (0.3512)\tPrec 89.844% (89.844%)\n",
            " * Prec 87.810% \n",
            "best acc: 88.180000\n",
            "Epoch: [7][0/391]\tTime 0.260 (0.260)\tData 0.140 (0.140)\tLoss 0.2776 (0.2776)\tPrec 90.625% (90.625%)\n",
            "Epoch: [7][100/391]\tTime 0.077 (0.070)\tData 0.002 (0.004)\tLoss 0.2136 (0.1867)\tPrec 93.750% (93.464%)\n",
            "Epoch: [7][200/391]\tTime 0.082 (0.074)\tData 0.001 (0.004)\tLoss 0.1670 (0.1820)\tPrec 92.969% (93.478%)\n",
            "Epoch: [7][300/391]\tTime 0.066 (0.072)\tData 0.004 (0.004)\tLoss 0.2877 (0.1861)\tPrec 90.625% (93.358%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.155 (0.155)\tLoss 0.2674 (0.2674)\tPrec 92.188% (92.188%)\n",
            " * Prec 88.210% \n",
            "best acc: 88.210000\n",
            "Epoch: [8][0/391]\tTime 0.259 (0.259)\tData 0.143 (0.143)\tLoss 0.2666 (0.2666)\tPrec 91.406% (91.406%)\n",
            "Epoch: [8][100/391]\tTime 0.116 (0.080)\tData 0.006 (0.005)\tLoss 0.1834 (0.1818)\tPrec 94.531% (93.696%)\n",
            "Epoch: [8][200/391]\tTime 0.058 (0.075)\tData 0.002 (0.004)\tLoss 0.1492 (0.1821)\tPrec 93.750% (93.606%)\n",
            "Epoch: [8][300/391]\tTime 0.077 (0.076)\tData 0.003 (0.004)\tLoss 0.1235 (0.1807)\tPrec 96.094% (93.675%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.168 (0.168)\tLoss 0.2396 (0.2396)\tPrec 92.969% (92.969%)\n",
            " * Prec 88.160% \n",
            "best acc: 88.210000\n",
            "Epoch: [9][0/391]\tTime 0.228 (0.228)\tData 0.134 (0.134)\tLoss 0.2156 (0.2156)\tPrec 93.750% (93.750%)\n",
            "Epoch: [9][100/391]\tTime 0.065 (0.080)\tData 0.002 (0.005)\tLoss 0.1963 (0.1792)\tPrec 90.625% (93.781%)\n",
            "Epoch: [9][200/391]\tTime 0.116 (0.076)\tData 0.005 (0.004)\tLoss 0.1299 (0.1782)\tPrec 96.094% (93.762%)\n",
            "Epoch: [9][300/391]\tTime 0.068 (0.076)\tData 0.002 (0.004)\tLoss 0.1903 (0.1806)\tPrec 94.531% (93.683%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.169 (0.169)\tLoss 0.2596 (0.2596)\tPrec 92.188% (92.188%)\n",
            " * Prec 88.030% \n",
            "best acc: 88.210000\n",
            "Epoch: [10][0/391]\tTime 0.236 (0.236)\tData 0.137 (0.137)\tLoss 0.1263 (0.1263)\tPrec 96.094% (96.094%)\n",
            "Epoch: [10][100/391]\tTime 0.062 (0.069)\tData 0.002 (0.004)\tLoss 0.1943 (0.1852)\tPrec 92.188% (93.294%)\n",
            "Epoch: [10][200/391]\tTime 0.060 (0.074)\tData 0.002 (0.004)\tLoss 0.1503 (0.1853)\tPrec 96.875% (93.404%)\n",
            "Epoch: [10][300/391]\tTime 0.067 (0.072)\tData 0.002 (0.003)\tLoss 0.2660 (0.1829)\tPrec 92.188% (93.529%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.157 (0.157)\tLoss 0.2709 (0.2709)\tPrec 93.750% (93.750%)\n",
            " * Prec 88.090% \n",
            "best acc: 88.210000\n",
            "Epoch: [11][0/391]\tTime 0.230 (0.230)\tData 0.136 (0.136)\tLoss 0.2116 (0.2116)\tPrec 92.188% (92.188%)\n",
            "Epoch: [11][100/391]\tTime 0.068 (0.082)\tData 0.002 (0.005)\tLoss 0.1065 (0.1798)\tPrec 95.312% (93.526%)\n",
            "Epoch: [11][200/391]\tTime 0.079 (0.074)\tData 0.005 (0.004)\tLoss 0.1431 (0.1756)\tPrec 95.312% (93.804%)\n",
            "Epoch: [11][300/391]\tTime 0.104 (0.076)\tData 0.005 (0.004)\tLoss 0.2223 (0.1772)\tPrec 90.625% (93.750%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.155 (0.155)\tLoss 0.2779 (0.2779)\tPrec 91.406% (91.406%)\n",
            " * Prec 88.000% \n",
            "best acc: 88.210000\n",
            "Epoch: [12][0/391]\tTime 0.404 (0.404)\tData 0.244 (0.244)\tLoss 0.1721 (0.1721)\tPrec 93.750% (93.750%)\n",
            "Epoch: [12][100/391]\tTime 0.072 (0.081)\tData 0.004 (0.006)\tLoss 0.2113 (0.1768)\tPrec 91.406% (93.688%)\n",
            "Epoch: [12][200/391]\tTime 0.110 (0.078)\tData 0.004 (0.005)\tLoss 0.2467 (0.1758)\tPrec 89.844% (93.738%)\n",
            "Epoch: [12][300/391]\tTime 0.072 (0.076)\tData 0.002 (0.004)\tLoss 0.1761 (0.1791)\tPrec 90.625% (93.605%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.209 (0.209)\tLoss 0.2689 (0.2689)\tPrec 92.188% (92.188%)\n",
            " * Prec 88.190% \n",
            "best acc: 88.210000\n",
            "Epoch: [13][0/391]\tTime 0.242 (0.242)\tData 0.140 (0.140)\tLoss 0.1657 (0.1657)\tPrec 95.312% (95.312%)\n",
            "Epoch: [13][100/391]\tTime 0.061 (0.070)\tData 0.002 (0.004)\tLoss 0.1165 (0.1724)\tPrec 95.312% (93.936%)\n",
            "Epoch: [13][200/391]\tTime 0.074 (0.075)\tData 0.004 (0.004)\tLoss 0.1003 (0.1700)\tPrec 95.312% (94.034%)\n",
            "Epoch: [13][300/391]\tTime 0.103 (0.074)\tData 0.007 (0.004)\tLoss 0.1817 (0.1722)\tPrec 94.531% (93.952%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.155 (0.155)\tLoss 0.2894 (0.2894)\tPrec 87.500% (87.500%)\n",
            " * Prec 88.160% \n",
            "best acc: 88.210000\n",
            "Epoch: [14][0/391]\tTime 0.257 (0.257)\tData 0.140 (0.140)\tLoss 0.1692 (0.1692)\tPrec 92.969% (92.969%)\n",
            "Epoch: [14][100/391]\tTime 0.074 (0.080)\tData 0.002 (0.005)\tLoss 0.1018 (0.1729)\tPrec 97.656% (93.812%)\n",
            "Epoch: [14][200/391]\tTime 0.063 (0.074)\tData 0.004 (0.004)\tLoss 0.2288 (0.1734)\tPrec 93.750% (93.855%)\n",
            "Epoch: [14][300/391]\tTime 0.064 (0.076)\tData 0.004 (0.004)\tLoss 0.1214 (0.1737)\tPrec 95.312% (93.843%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.132 (0.132)\tLoss 0.2347 (0.2347)\tPrec 93.750% (93.750%)\n",
            " * Prec 88.260% \n",
            "best acc: 88.260000\n",
            "Epoch: [15][0/391]\tTime 0.398 (0.398)\tData 0.219 (0.219)\tLoss 0.1461 (0.1461)\tPrec 95.312% (95.312%)\n",
            "Epoch: [15][100/391]\tTime 0.066 (0.076)\tData 0.003 (0.005)\tLoss 0.0940 (0.1688)\tPrec 97.656% (94.075%)\n",
            "Epoch: [15][200/391]\tTime 0.079 (0.076)\tData 0.004 (0.005)\tLoss 0.1373 (0.1718)\tPrec 94.531% (94.042%)\n",
            "Epoch: [15][300/391]\tTime 0.061 (0.074)\tData 0.004 (0.004)\tLoss 0.1557 (0.1719)\tPrec 92.969% (93.997%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.212 (0.212)\tLoss 0.2356 (0.2356)\tPrec 89.062% (89.062%)\n",
            " * Prec 88.570% \n",
            "best acc: 88.570000\n",
            "Epoch: [16][0/391]\tTime 0.228 (0.228)\tData 0.134 (0.134)\tLoss 0.1521 (0.1521)\tPrec 93.750% (93.750%)\n",
            "Epoch: [16][100/391]\tTime 0.067 (0.068)\tData 0.003 (0.004)\tLoss 0.1220 (0.1643)\tPrec 96.094% (94.183%)\n",
            "Epoch: [16][200/391]\tTime 0.050 (0.074)\tData 0.002 (0.004)\tLoss 0.2167 (0.1654)\tPrec 91.406% (94.088%)\n",
            "Epoch: [16][300/391]\tTime 0.079 (0.072)\tData 0.002 (0.004)\tLoss 0.2287 (0.1685)\tPrec 92.188% (93.981%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.157 (0.157)\tLoss 0.2390 (0.2390)\tPrec 93.750% (93.750%)\n",
            " * Prec 88.080% \n",
            "best acc: 88.570000\n",
            "Epoch: [17][0/391]\tTime 0.225 (0.225)\tData 0.132 (0.132)\tLoss 0.2465 (0.2465)\tPrec 90.625% (90.625%)\n",
            "Epoch: [17][100/391]\tTime 0.073 (0.080)\tData 0.005 (0.005)\tLoss 0.1952 (0.1597)\tPrec 94.531% (94.585%)\n",
            "Epoch: [17][200/391]\tTime 0.123 (0.074)\tData 0.004 (0.004)\tLoss 0.1795 (0.1667)\tPrec 93.750% (94.201%)\n",
            "Epoch: [17][300/391]\tTime 0.058 (0.075)\tData 0.002 (0.004)\tLoss 0.1863 (0.1660)\tPrec 91.406% (94.194%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.161 (0.161)\tLoss 0.2572 (0.2572)\tPrec 91.406% (91.406%)\n",
            " * Prec 88.580% \n",
            "best acc: 88.580000\n",
            "Epoch: [18][0/391]\tTime 0.363 (0.363)\tData 0.235 (0.235)\tLoss 0.2187 (0.2187)\tPrec 92.969% (92.969%)\n",
            "Epoch: [18][100/391]\tTime 0.058 (0.077)\tData 0.003 (0.006)\tLoss 0.1628 (0.1575)\tPrec 94.531% (94.624%)\n",
            "Epoch: [18][200/391]\tTime 0.096 (0.077)\tData 0.002 (0.004)\tLoss 0.1971 (0.1644)\tPrec 92.969% (94.271%)\n",
            "Epoch: [18][300/391]\tTime 0.063 (0.074)\tData 0.004 (0.004)\tLoss 0.2459 (0.1669)\tPrec 89.844% (94.134%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.285 (0.285)\tLoss 0.3136 (0.3136)\tPrec 86.719% (86.719%)\n",
            " * Prec 87.680% \n",
            "best acc: 88.580000\n",
            "Epoch: [19][0/391]\tTime 0.231 (0.231)\tData 0.138 (0.138)\tLoss 0.1511 (0.1511)\tPrec 93.750% (93.750%)\n",
            "Epoch: [19][100/391]\tTime 0.061 (0.070)\tData 0.002 (0.004)\tLoss 0.1308 (0.1595)\tPrec 96.094% (94.516%)\n",
            "Epoch: [19][200/391]\tTime 0.048 (0.086)\tData 0.002 (0.004)\tLoss 0.2214 (0.1603)\tPrec 89.844% (94.384%)\n",
            "Epoch: [19][300/391]\tTime 0.062 (0.089)\tData 0.002 (0.004)\tLoss 0.2534 (0.1615)\tPrec 92.188% (94.318%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.155 (0.155)\tLoss 0.3267 (0.3267)\tPrec 89.844% (89.844%)\n",
            " * Prec 87.890% \n",
            "best acc: 88.580000\n",
            "Epoch: [20][0/391]\tTime 0.360 (0.360)\tData 0.203 (0.203)\tLoss 0.2052 (0.2052)\tPrec 92.969% (92.969%)\n",
            "Epoch: [20][100/391]\tTime 0.060 (0.081)\tData 0.002 (0.005)\tLoss 0.1601 (0.1615)\tPrec 92.969% (94.106%)\n",
            "Epoch: [20][200/391]\tTime 0.083 (0.077)\tData 0.004 (0.004)\tLoss 0.1957 (0.1599)\tPrec 92.188% (94.193%)\n",
            "Epoch: [20][300/391]\tTime 0.062 (0.076)\tData 0.002 (0.004)\tLoss 0.1723 (0.1597)\tPrec 93.750% (94.365%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.186 (0.186)\tLoss 0.3043 (0.3043)\tPrec 91.406% (91.406%)\n",
            " * Prec 88.030% \n",
            "best acc: 88.580000\n",
            "Epoch: [21][0/391]\tTime 0.248 (0.248)\tData 0.138 (0.138)\tLoss 0.1249 (0.1249)\tPrec 96.094% (96.094%)\n",
            "Epoch: [21][100/391]\tTime 0.061 (0.071)\tData 0.002 (0.005)\tLoss 0.2079 (0.1636)\tPrec 95.312% (94.137%)\n",
            "Epoch: [21][200/391]\tTime 0.061 (0.075)\tData 0.002 (0.004)\tLoss 0.1988 (0.1623)\tPrec 92.969% (94.279%)\n",
            "Epoch: [21][300/391]\tTime 0.072 (0.073)\tData 0.002 (0.004)\tLoss 0.1081 (0.1600)\tPrec 97.656% (94.329%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.155 (0.155)\tLoss 0.3099 (0.3099)\tPrec 89.844% (89.844%)\n",
            " * Prec 88.390% \n",
            "best acc: 88.580000\n",
            "Epoch: [22][0/391]\tTime 0.246 (0.246)\tData 0.143 (0.143)\tLoss 0.1770 (0.1770)\tPrec 94.531% (94.531%)\n",
            "Epoch: [22][100/391]\tTime 0.066 (0.082)\tData 0.002 (0.006)\tLoss 0.2333 (0.1607)\tPrec 92.969% (94.353%)\n",
            "Epoch: [22][200/391]\tTime 0.066 (0.075)\tData 0.004 (0.004)\tLoss 0.2314 (0.1577)\tPrec 92.188% (94.407%)\n",
            "Epoch: [22][300/391]\tTime 0.060 (0.076)\tData 0.002 (0.004)\tLoss 0.1215 (0.1586)\tPrec 96.094% (94.420%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.157 (0.157)\tLoss 0.2801 (0.2801)\tPrec 89.844% (89.844%)\n",
            " * Prec 87.890% \n",
            "best acc: 88.580000\n",
            "Epoch: [23][0/391]\tTime 0.379 (0.379)\tData 0.223 (0.223)\tLoss 0.1136 (0.1136)\tPrec 96.094% (96.094%)\n",
            "Epoch: [23][100/391]\tTime 0.062 (0.079)\tData 0.002 (0.005)\tLoss 0.1360 (0.1575)\tPrec 95.312% (94.361%)\n",
            "Epoch: [23][200/391]\tTime 0.086 (0.077)\tData 0.004 (0.005)\tLoss 0.2028 (0.1580)\tPrec 92.188% (94.395%)\n",
            "Epoch: [23][300/391]\tTime 0.058 (0.075)\tData 0.002 (0.004)\tLoss 0.1819 (0.1582)\tPrec 94.531% (94.339%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.248 (0.248)\tLoss 0.2502 (0.2502)\tPrec 89.844% (89.844%)\n",
            " * Prec 88.440% \n",
            "best acc: 88.580000\n",
            "Epoch: [24][0/391]\tTime 0.243 (0.243)\tData 0.150 (0.150)\tLoss 0.2264 (0.2264)\tPrec 92.188% (92.188%)\n",
            "Epoch: [24][100/391]\tTime 0.114 (0.074)\tData 0.004 (0.004)\tLoss 0.1758 (0.1505)\tPrec 92.969% (94.694%)\n",
            "Epoch: [24][200/391]\tTime 0.067 (0.078)\tData 0.003 (0.004)\tLoss 0.1135 (0.1547)\tPrec 96.094% (94.520%)\n",
            "Epoch: [24][300/391]\tTime 0.080 (0.079)\tData 0.003 (0.004)\tLoss 0.2613 (0.1552)\tPrec 92.969% (94.510%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.161 (0.161)\tLoss 0.2488 (0.2488)\tPrec 92.969% (92.969%)\n",
            " * Prec 88.200% \n",
            "best acc: 88.580000\n",
            "Epoch: [25][0/391]\tTime 0.264 (0.264)\tData 0.137 (0.137)\tLoss 0.1861 (0.1861)\tPrec 92.969% (92.969%)\n",
            "Epoch: [25][100/391]\tTime 0.060 (0.082)\tData 0.002 (0.005)\tLoss 0.1287 (0.1510)\tPrec 94.531% (94.717%)\n",
            "Epoch: [25][200/391]\tTime 0.075 (0.075)\tData 0.004 (0.004)\tLoss 0.1312 (0.1547)\tPrec 93.750% (94.391%)\n",
            "Epoch: [25][300/391]\tTime 0.061 (0.077)\tData 0.002 (0.004)\tLoss 0.2365 (0.1555)\tPrec 90.625% (94.344%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.227 (0.227)\tLoss 0.3382 (0.3382)\tPrec 85.938% (85.938%)\n",
            " * Prec 87.800% \n",
            "best acc: 88.580000\n",
            "Epoch: [26][0/391]\tTime 0.257 (0.257)\tData 0.145 (0.145)\tLoss 0.1417 (0.1417)\tPrec 93.750% (93.750%)\n",
            "Epoch: [26][100/391]\tTime 0.065 (0.070)\tData 0.004 (0.004)\tLoss 0.1811 (0.1456)\tPrec 94.531% (94.779%)\n",
            "Epoch: [26][200/391]\tTime 0.061 (0.075)\tData 0.002 (0.004)\tLoss 0.0991 (0.1502)\tPrec 96.875% (94.640%)\n",
            "Epoch: [26][300/391]\tTime 0.054 (0.072)\tData 0.002 (0.003)\tLoss 0.1309 (0.1493)\tPrec 95.312% (94.671%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.158 (0.158)\tLoss 0.2933 (0.2933)\tPrec 91.406% (91.406%)\n",
            " * Prec 88.390% \n",
            "best acc: 88.580000\n",
            "Epoch: [27][0/391]\tTime 0.266 (0.266)\tData 0.158 (0.158)\tLoss 0.1378 (0.1378)\tPrec 95.312% (95.312%)\n",
            "Epoch: [27][100/391]\tTime 0.114 (0.078)\tData 0.005 (0.005)\tLoss 0.1274 (0.1410)\tPrec 92.969% (95.150%)\n",
            "Epoch: [27][200/391]\tTime 0.077 (0.075)\tData 0.002 (0.004)\tLoss 0.0781 (0.1496)\tPrec 97.656% (94.776%)\n",
            "Epoch: [27][300/391]\tTime 0.060 (0.076)\tData 0.002 (0.004)\tLoss 0.0895 (0.1507)\tPrec 96.875% (94.682%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.170 (0.170)\tLoss 0.2775 (0.2775)\tPrec 90.625% (90.625%)\n",
            " * Prec 88.060% \n",
            "best acc: 88.580000\n",
            "Epoch: [28][0/391]\tTime 0.241 (0.241)\tData 0.138 (0.138)\tLoss 0.2304 (0.2304)\tPrec 89.062% (89.062%)\n",
            "Epoch: [28][100/391]\tTime 0.067 (0.084)\tData 0.002 (0.005)\tLoss 0.1508 (0.1530)\tPrec 95.312% (94.516%)\n",
            "Epoch: [28][200/391]\tTime 0.091 (0.079)\tData 0.001 (0.004)\tLoss 0.1062 (0.1536)\tPrec 95.312% (94.477%)\n",
            "Epoch: [28][300/391]\tTime 0.061 (0.078)\tData 0.002 (0.004)\tLoss 0.1922 (0.1511)\tPrec 93.750% (94.573%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.194 (0.194)\tLoss 0.3005 (0.3005)\tPrec 89.844% (89.844%)\n",
            " * Prec 88.370% \n",
            "best acc: 88.580000\n",
            "Epoch: [29][0/391]\tTime 0.253 (0.253)\tData 0.147 (0.147)\tLoss 0.1270 (0.1270)\tPrec 94.531% (94.531%)\n",
            "Epoch: [29][100/391]\tTime 0.063 (0.070)\tData 0.002 (0.004)\tLoss 0.2734 (0.1456)\tPrec 92.969% (94.903%)\n",
            "Epoch: [29][200/391]\tTime 0.062 (0.075)\tData 0.002 (0.004)\tLoss 0.1849 (0.1487)\tPrec 92.969% (94.869%)\n",
            "Epoch: [29][300/391]\tTime 0.060 (0.073)\tData 0.002 (0.004)\tLoss 0.1240 (0.1479)\tPrec 96.094% (94.840%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.159 (0.159)\tLoss 0.2697 (0.2697)\tPrec 92.188% (92.188%)\n",
            " * Prec 88.020% \n",
            "best acc: 88.580000\n",
            "Epoch: [30][0/391]\tTime 0.231 (0.231)\tData 0.132 (0.132)\tLoss 0.1559 (0.1559)\tPrec 93.750% (93.750%)\n",
            "Epoch: [30][100/391]\tTime 0.082 (0.081)\tData 0.002 (0.005)\tLoss 0.1916 (0.1409)\tPrec 92.969% (94.988%)\n",
            "Epoch: [30][200/391]\tTime 0.065 (0.074)\tData 0.004 (0.004)\tLoss 0.1772 (0.1449)\tPrec 96.094% (94.850%)\n",
            "Epoch: [30][300/391]\tTime 0.073 (0.076)\tData 0.002 (0.004)\tLoss 0.1461 (0.1476)\tPrec 93.750% (94.757%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.136 (0.136)\tLoss 0.3243 (0.3243)\tPrec 91.406% (91.406%)\n",
            " * Prec 88.130% \n",
            "best acc: 88.580000\n",
            "Epoch: [31][0/391]\tTime 0.391 (0.391)\tData 0.248 (0.248)\tLoss 0.1340 (0.1340)\tPrec 93.750% (93.750%)\n",
            "Epoch: [31][100/391]\tTime 0.063 (0.078)\tData 0.002 (0.006)\tLoss 0.1640 (0.1370)\tPrec 95.312% (95.065%)\n",
            "Epoch: [31][200/391]\tTime 0.125 (0.078)\tData 0.006 (0.005)\tLoss 0.1390 (0.1391)\tPrec 94.531% (95.040%)\n",
            "Epoch: [31][300/391]\tTime 0.062 (0.075)\tData 0.004 (0.004)\tLoss 0.1553 (0.1427)\tPrec 95.312% (94.908%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.266 (0.266)\tLoss 0.3563 (0.3563)\tPrec 90.625% (90.625%)\n",
            " * Prec 88.190% \n",
            "best acc: 88.580000\n",
            "Epoch: [32][0/391]\tTime 0.234 (0.234)\tData 0.138 (0.138)\tLoss 0.1310 (0.1310)\tPrec 96.094% (96.094%)\n",
            "Epoch: [32][100/391]\tTime 0.097 (0.070)\tData 0.004 (0.004)\tLoss 0.1581 (0.1371)\tPrec 92.969% (94.926%)\n",
            "Epoch: [32][200/391]\tTime 0.069 (0.075)\tData 0.002 (0.004)\tLoss 0.0831 (0.1417)\tPrec 97.656% (94.877%)\n",
            "Epoch: [32][300/391]\tTime 0.058 (0.075)\tData 0.002 (0.004)\tLoss 0.1733 (0.1462)\tPrec 94.531% (94.726%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.159 (0.159)\tLoss 0.3327 (0.3327)\tPrec 88.281% (88.281%)\n",
            " * Prec 88.120% \n",
            "best acc: 88.580000\n",
            "Epoch: [33][0/391]\tTime 0.280 (0.280)\tData 0.156 (0.156)\tLoss 0.1220 (0.1220)\tPrec 96.094% (96.094%)\n",
            "Epoch: [33][100/391]\tTime 0.076 (0.083)\tData 0.004 (0.005)\tLoss 0.0793 (0.1323)\tPrec 96.094% (95.436%)\n",
            "Epoch: [33][200/391]\tTime 0.064 (0.076)\tData 0.002 (0.004)\tLoss 0.0855 (0.1390)\tPrec 98.438% (95.118%)\n",
            "Epoch: [33][300/391]\tTime 0.050 (0.078)\tData 0.002 (0.004)\tLoss 0.1801 (0.1394)\tPrec 92.969% (95.027%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.175 (0.175)\tLoss 0.2352 (0.2352)\tPrec 92.188% (92.188%)\n",
            " * Prec 87.800% \n",
            "best acc: 88.580000\n",
            "Epoch: [34][0/391]\tTime 0.255 (0.255)\tData 0.134 (0.134)\tLoss 0.1284 (0.1284)\tPrec 93.750% (93.750%)\n",
            "Epoch: [34][100/391]\tTime 0.065 (0.071)\tData 0.002 (0.004)\tLoss 0.2162 (0.1375)\tPrec 92.188% (95.181%)\n",
            "Epoch: [34][200/391]\tTime 0.071 (0.076)\tData 0.002 (0.004)\tLoss 0.0684 (0.1372)\tPrec 97.656% (95.184%)\n",
            "Epoch: [34][300/391]\tTime 0.064 (0.073)\tData 0.002 (0.004)\tLoss 0.1022 (0.1415)\tPrec 97.656% (95.030%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.161 (0.161)\tLoss 0.4328 (0.4328)\tPrec 91.406% (91.406%)\n",
            " * Prec 88.350% \n",
            "best acc: 88.580000\n",
            "Epoch: [35][0/391]\tTime 0.240 (0.240)\tData 0.138 (0.138)\tLoss 0.1166 (0.1166)\tPrec 95.312% (95.312%)\n",
            "Epoch: [35][100/391]\tTime 0.105 (0.080)\tData 0.004 (0.005)\tLoss 0.0850 (0.1351)\tPrec 96.094% (95.374%)\n",
            "Epoch: [35][200/391]\tTime 0.073 (0.076)\tData 0.002 (0.004)\tLoss 0.0730 (0.1330)\tPrec 98.438% (95.410%)\n",
            "Epoch: [35][300/391]\tTime 0.098 (0.078)\tData 0.004 (0.004)\tLoss 0.1128 (0.1339)\tPrec 98.438% (95.338%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.170 (0.170)\tLoss 0.4073 (0.4073)\tPrec 92.188% (92.188%)\n",
            " * Prec 88.080% \n",
            "best acc: 88.580000\n",
            "Epoch: [36][0/391]\tTime 0.232 (0.232)\tData 0.138 (0.138)\tLoss 0.0791 (0.0791)\tPrec 97.656% (97.656%)\n",
            "Epoch: [36][100/391]\tTime 0.066 (0.084)\tData 0.004 (0.005)\tLoss 0.1098 (0.1316)\tPrec 94.531% (95.336%)\n",
            "Epoch: [36][200/391]\tTime 0.098 (0.078)\tData 0.004 (0.004)\tLoss 0.1305 (0.1329)\tPrec 94.531% (95.309%)\n",
            "Epoch: [36][300/391]\tTime 0.061 (0.077)\tData 0.004 (0.004)\tLoss 0.1769 (0.1350)\tPrec 92.188% (95.203%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.273 (0.273)\tLoss 0.2666 (0.2666)\tPrec 89.844% (89.844%)\n",
            " * Prec 88.070% \n",
            "best acc: 88.580000\n",
            "Epoch: [37][0/391]\tTime 0.237 (0.237)\tData 0.144 (0.144)\tLoss 0.1814 (0.1814)\tPrec 92.188% (92.188%)\n",
            "Epoch: [37][100/391]\tTime 0.064 (0.070)\tData 0.004 (0.005)\tLoss 0.1404 (0.1375)\tPrec 92.188% (95.073%)\n",
            "Epoch: [37][200/391]\tTime 0.071 (0.076)\tData 0.002 (0.004)\tLoss 0.1059 (0.1358)\tPrec 96.094% (95.169%)\n",
            "Epoch: [37][300/391]\tTime 0.151 (0.074)\tData 0.009 (0.004)\tLoss 0.1775 (0.1358)\tPrec 94.531% (95.198%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.171 (0.171)\tLoss 0.2726 (0.2726)\tPrec 92.969% (92.969%)\n",
            " * Prec 88.340% \n",
            "best acc: 88.580000\n",
            "Epoch: [38][0/391]\tTime 0.243 (0.243)\tData 0.145 (0.145)\tLoss 0.0645 (0.0645)\tPrec 98.438% (98.438%)\n",
            "Epoch: [38][100/391]\tTime 0.066 (0.083)\tData 0.002 (0.005)\tLoss 0.1008 (0.1289)\tPrec 96.875% (95.614%)\n",
            "Epoch: [38][200/391]\tTime 0.069 (0.076)\tData 0.003 (0.004)\tLoss 0.1405 (0.1321)\tPrec 95.312% (95.332%)\n",
            "Epoch: [38][300/391]\tTime 0.064 (0.078)\tData 0.002 (0.004)\tLoss 0.1110 (0.1379)\tPrec 94.531% (95.159%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.163 (0.163)\tLoss 0.3419 (0.3419)\tPrec 88.281% (88.281%)\n",
            " * Prec 88.250% \n",
            "best acc: 88.580000\n",
            "Epoch: [39][0/391]\tTime 0.423 (0.423)\tData 0.211 (0.211)\tLoss 0.1149 (0.1149)\tPrec 96.094% (96.094%)\n",
            "Epoch: [39][100/391]\tTime 0.066 (0.075)\tData 0.004 (0.005)\tLoss 0.1498 (0.1358)\tPrec 93.750% (95.173%)\n",
            "Epoch: [39][200/391]\tTime 0.057 (0.077)\tData 0.003 (0.004)\tLoss 0.1595 (0.1352)\tPrec 93.750% (95.180%)\n",
            "Epoch: [39][300/391]\tTime 0.064 (0.074)\tData 0.004 (0.004)\tLoss 0.1686 (0.1357)\tPrec 95.312% (95.107%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.166 (0.166)\tLoss 0.3527 (0.3527)\tPrec 90.625% (90.625%)\n",
            " * Prec 88.150% \n",
            "best acc: 88.580000\n",
            "Epoch: [40][0/391]\tTime 0.246 (0.246)\tData 0.142 (0.142)\tLoss 0.1259 (0.1259)\tPrec 94.531% (94.531%)\n",
            "Epoch: [40][100/391]\tTime 0.064 (0.072)\tData 0.002 (0.005)\tLoss 0.0858 (0.1329)\tPrec 97.656% (95.312%)\n",
            "Epoch: [40][200/391]\tTime 0.103 (0.074)\tData 0.003 (0.004)\tLoss 0.2288 (0.1341)\tPrec 92.188% (95.215%)\n",
            "Epoch: [40][300/391]\tTime 0.127 (0.074)\tData 0.003 (0.004)\tLoss 0.1323 (0.1335)\tPrec 95.312% (95.242%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.134 (0.134)\tLoss 0.3246 (0.3246)\tPrec 89.844% (89.844%)\n",
            " * Prec 88.090% \n",
            "best acc: 88.580000\n",
            "Epoch: [41][0/391]\tTime 0.231 (0.231)\tData 0.138 (0.138)\tLoss 0.0651 (0.0651)\tPrec 97.656% (97.656%)\n",
            "Epoch: [41][100/391]\tTime 0.070 (0.082)\tData 0.002 (0.005)\tLoss 0.1810 (0.1374)\tPrec 93.750% (95.158%)\n",
            "Epoch: [41][200/391]\tTime 0.065 (0.075)\tData 0.002 (0.004)\tLoss 0.2190 (0.1372)\tPrec 92.188% (95.079%)\n",
            "Epoch: [41][300/391]\tTime 0.063 (0.077)\tData 0.004 (0.004)\tLoss 0.1221 (0.1354)\tPrec 96.875% (95.183%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.157 (0.157)\tLoss 0.3715 (0.3715)\tPrec 89.844% (89.844%)\n",
            " * Prec 88.020% \n",
            "best acc: 88.580000\n",
            "Epoch: [42][0/391]\tTime 0.223 (0.223)\tData 0.138 (0.138)\tLoss 0.0964 (0.0964)\tPrec 95.312% (95.312%)\n",
            "Epoch: [42][100/391]\tTime 0.077 (0.070)\tData 0.004 (0.004)\tLoss 0.0675 (0.1382)\tPrec 96.875% (95.212%)\n",
            "Epoch: [42][200/391]\tTime 0.072 (0.075)\tData 0.003 (0.004)\tLoss 0.0938 (0.1341)\tPrec 96.875% (95.196%)\n",
            "Epoch: [42][300/391]\tTime 0.062 (0.073)\tData 0.002 (0.004)\tLoss 0.0939 (0.1361)\tPrec 97.656% (95.110%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.171 (0.171)\tLoss 0.3364 (0.3364)\tPrec 89.844% (89.844%)\n",
            " * Prec 87.760% \n",
            "best acc: 88.580000\n",
            "Epoch: [43][0/391]\tTime 0.234 (0.234)\tData 0.135 (0.135)\tLoss 0.1142 (0.1142)\tPrec 96.875% (96.875%)\n",
            "Epoch: [43][100/391]\tTime 0.067 (0.079)\tData 0.003 (0.005)\tLoss 0.0732 (0.1379)\tPrec 98.438% (95.297%)\n",
            "Epoch: [43][200/391]\tTime 0.089 (0.075)\tData 0.005 (0.004)\tLoss 0.1308 (0.1356)\tPrec 93.750% (95.208%)\n",
            "Epoch: [43][300/391]\tTime 0.062 (0.077)\tData 0.003 (0.004)\tLoss 0.1554 (0.1339)\tPrec 93.750% (95.248%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.158 (0.158)\tLoss 0.2944 (0.2944)\tPrec 90.625% (90.625%)\n",
            " * Prec 88.290% \n",
            "best acc: 88.580000\n",
            "Epoch: [44][0/391]\tTime 0.238 (0.238)\tData 0.143 (0.143)\tLoss 0.1627 (0.1627)\tPrec 96.094% (96.094%)\n",
            "Epoch: [44][100/391]\tTime 0.075 (0.082)\tData 0.003 (0.005)\tLoss 0.1029 (0.1423)\tPrec 96.875% (95.127%)\n",
            "Epoch: [44][200/391]\tTime 0.112 (0.076)\tData 0.002 (0.004)\tLoss 0.0730 (0.1345)\tPrec 96.875% (95.274%)\n",
            "Epoch: [44][300/391]\tTime 0.063 (0.077)\tData 0.004 (0.004)\tLoss 0.1453 (0.1353)\tPrec 96.094% (95.165%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.271 (0.271)\tLoss 0.3395 (0.3395)\tPrec 88.281% (88.281%)\n",
            " * Prec 88.470% \n",
            "best acc: 88.580000\n",
            "Epoch: [45][0/391]\tTime 0.229 (0.229)\tData 0.141 (0.141)\tLoss 0.1600 (0.1600)\tPrec 96.094% (96.094%)\n",
            "Epoch: [45][100/391]\tTime 0.107 (0.069)\tData 0.002 (0.004)\tLoss 0.0692 (0.1239)\tPrec 96.875% (95.699%)\n",
            "Epoch: [45][200/391]\tTime 0.063 (0.075)\tData 0.002 (0.004)\tLoss 0.1240 (0.1278)\tPrec 97.656% (95.476%)\n",
            "Epoch: [45][300/391]\tTime 0.093 (0.073)\tData 0.002 (0.004)\tLoss 0.1100 (0.1270)\tPrec 96.094% (95.515%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.165 (0.165)\tLoss 0.3201 (0.3201)\tPrec 90.625% (90.625%)\n",
            " * Prec 88.270% \n",
            "best acc: 88.580000\n",
            "Epoch: [46][0/391]\tTime 0.250 (0.250)\tData 0.155 (0.155)\tLoss 0.0700 (0.0700)\tPrec 97.656% (97.656%)\n",
            "Epoch: [46][100/391]\tTime 0.070 (0.082)\tData 0.004 (0.005)\tLoss 0.1951 (0.1258)\tPrec 93.750% (95.475%)\n",
            "Epoch: [46][200/391]\tTime 0.065 (0.075)\tData 0.003 (0.004)\tLoss 0.1857 (0.1240)\tPrec 93.750% (95.600%)\n",
            "Epoch: [46][300/391]\tTime 0.085 (0.077)\tData 0.002 (0.004)\tLoss 0.1301 (0.1263)\tPrec 96.094% (95.510%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.152 (0.152)\tLoss 0.3040 (0.3040)\tPrec 90.625% (90.625%)\n",
            " * Prec 88.070% \n",
            "best acc: 88.580000\n",
            "Epoch: [47][0/391]\tTime 0.389 (0.389)\tData 0.232 (0.232)\tLoss 0.1478 (0.1478)\tPrec 96.094% (96.094%)\n",
            "Epoch: [47][100/391]\tTime 0.060 (0.081)\tData 0.004 (0.006)\tLoss 0.1025 (0.1217)\tPrec 96.875% (95.753%)\n",
            "Epoch: [47][200/391]\tTime 0.115 (0.079)\tData 0.004 (0.005)\tLoss 0.1086 (0.1258)\tPrec 95.312% (95.522%)\n",
            "Epoch: [47][300/391]\tTime 0.062 (0.076)\tData 0.003 (0.004)\tLoss 0.1294 (0.1261)\tPrec 96.875% (95.531%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.225 (0.225)\tLoss 0.4205 (0.4205)\tPrec 89.062% (89.062%)\n",
            " * Prec 88.560% \n",
            "best acc: 88.580000\n",
            "Epoch: [48][0/391]\tTime 0.228 (0.228)\tData 0.139 (0.139)\tLoss 0.1976 (0.1976)\tPrec 90.625% (90.625%)\n",
            "Epoch: [48][100/391]\tTime 0.060 (0.070)\tData 0.002 (0.004)\tLoss 0.1273 (0.1271)\tPrec 94.531% (95.374%)\n",
            "Epoch: [48][200/391]\tTime 0.080 (0.075)\tData 0.004 (0.004)\tLoss 0.0655 (0.1240)\tPrec 97.656% (95.550%)\n",
            "Epoch: [48][300/391]\tTime 0.073 (0.074)\tData 0.004 (0.004)\tLoss 0.2374 (0.1244)\tPrec 92.188% (95.525%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.165 (0.165)\tLoss 0.4039 (0.4039)\tPrec 90.625% (90.625%)\n",
            " * Prec 88.240% \n",
            "best acc: 88.580000\n",
            "Epoch: [49][0/391]\tTime 0.235 (0.235)\tData 0.138 (0.138)\tLoss 0.1691 (0.1691)\tPrec 92.188% (92.188%)\n",
            "Epoch: [49][100/391]\tTime 0.085 (0.084)\tData 0.004 (0.005)\tLoss 0.1371 (0.1227)\tPrec 94.531% (95.568%)\n",
            "Epoch: [49][200/391]\tTime 0.066 (0.076)\tData 0.002 (0.004)\tLoss 0.0651 (0.1285)\tPrec 97.656% (95.464%)\n",
            "Epoch: [49][300/391]\tTime 0.062 (0.077)\tData 0.003 (0.004)\tLoss 0.0989 (0.1295)\tPrec 94.531% (95.344%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.177 (0.177)\tLoss 0.3859 (0.3859)\tPrec 89.062% (89.062%)\n",
            " * Prec 88.200% \n",
            "best acc: 88.580000\n",
            "Epoch: [50][0/391]\tTime 0.324 (0.324)\tData 0.178 (0.178)\tLoss 0.1763 (0.1763)\tPrec 93.750% (93.750%)\n",
            "Epoch: [50][100/391]\tTime 0.069 (0.076)\tData 0.002 (0.005)\tLoss 0.1472 (0.1245)\tPrec 95.312% (95.506%)\n",
            "Epoch: [50][200/391]\tTime 0.063 (0.079)\tData 0.002 (0.004)\tLoss 0.1946 (0.1245)\tPrec 93.750% (95.487%)\n",
            "Epoch: [50][300/391]\tTime 0.063 (0.075)\tData 0.002 (0.004)\tLoss 0.0643 (0.1245)\tPrec 99.219% (95.533%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.158 (0.158)\tLoss 0.3685 (0.3685)\tPrec 86.719% (86.719%)\n",
            " * Prec 88.030% \n",
            "best acc: 88.580000\n",
            "Epoch: [51][0/391]\tTime 0.233 (0.233)\tData 0.138 (0.138)\tLoss 0.1712 (0.1712)\tPrec 91.406% (91.406%)\n",
            "Epoch: [51][100/391]\tTime 0.083 (0.074)\tData 0.003 (0.004)\tLoss 0.1203 (0.1173)\tPrec 96.094% (95.692%)\n",
            "Epoch: [51][200/391]\tTime 0.065 (0.074)\tData 0.003 (0.004)\tLoss 0.1298 (0.1205)\tPrec 95.312% (95.767%)\n",
            "Epoch: [51][300/391]\tTime 0.079 (0.076)\tData 0.003 (0.004)\tLoss 0.1729 (0.1223)\tPrec 92.969% (95.743%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.135 (0.135)\tLoss 0.2736 (0.2736)\tPrec 90.625% (90.625%)\n",
            " * Prec 88.660% \n",
            "best acc: 88.660000\n",
            "Epoch: [52][0/391]\tTime 0.238 (0.238)\tData 0.146 (0.146)\tLoss 0.1501 (0.1501)\tPrec 93.750% (93.750%)\n",
            "Epoch: [52][100/391]\tTime 0.060 (0.082)\tData 0.002 (0.005)\tLoss 0.1178 (0.1170)\tPrec 95.312% (95.715%)\n",
            "Epoch: [52][200/391]\tTime 0.059 (0.075)\tData 0.004 (0.004)\tLoss 0.1106 (0.1170)\tPrec 96.094% (95.783%)\n",
            "Epoch: [52][300/391]\tTime 0.061 (0.076)\tData 0.002 (0.004)\tLoss 0.1910 (0.1172)\tPrec 94.531% (95.842%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.288 (0.288)\tLoss 0.3315 (0.3315)\tPrec 89.062% (89.062%)\n",
            " * Prec 88.520% \n",
            "best acc: 88.660000\n",
            "Epoch: [53][0/391]\tTime 0.237 (0.237)\tData 0.138 (0.138)\tLoss 0.1235 (0.1235)\tPrec 95.312% (95.312%)\n",
            "Epoch: [53][100/391]\tTime 0.061 (0.069)\tData 0.002 (0.004)\tLoss 0.1048 (0.1190)\tPrec 96.875% (95.846%)\n",
            "Epoch: [53][200/391]\tTime 0.074 (0.075)\tData 0.003 (0.004)\tLoss 0.1467 (0.1217)\tPrec 94.531% (95.713%)\n",
            "Epoch: [53][300/391]\tTime 0.062 (0.072)\tData 0.002 (0.003)\tLoss 0.1390 (0.1189)\tPrec 92.969% (95.777%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.156 (0.156)\tLoss 0.3458 (0.3458)\tPrec 89.062% (89.062%)\n",
            " * Prec 88.510% \n",
            "best acc: 88.660000\n",
            "Epoch: [54][0/391]\tTime 0.249 (0.249)\tData 0.156 (0.156)\tLoss 0.1393 (0.1393)\tPrec 94.531% (94.531%)\n",
            "Epoch: [54][100/391]\tTime 0.103 (0.077)\tData 0.002 (0.005)\tLoss 0.0790 (0.1163)\tPrec 98.438% (95.869%)\n",
            "Epoch: [54][200/391]\tTime 0.059 (0.075)\tData 0.004 (0.004)\tLoss 0.1348 (0.1178)\tPrec 96.875% (95.826%)\n",
            "Epoch: [54][300/391]\tTime 0.070 (0.076)\tData 0.005 (0.004)\tLoss 0.1834 (0.1214)\tPrec 94.531% (95.678%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.131 (0.131)\tLoss 0.4260 (0.4260)\tPrec 89.844% (89.844%)\n",
            " * Prec 87.730% \n",
            "best acc: 88.660000\n",
            "Epoch: [55][0/391]\tTime 0.263 (0.263)\tData 0.141 (0.141)\tLoss 0.1674 (0.1674)\tPrec 92.188% (92.188%)\n",
            "Epoch: [55][100/391]\tTime 0.070 (0.082)\tData 0.004 (0.005)\tLoss 0.1166 (0.1197)\tPrec 96.094% (95.668%)\n",
            "Epoch: [55][200/391]\tTime 0.109 (0.076)\tData 0.007 (0.004)\tLoss 0.0713 (0.1166)\tPrec 98.438% (95.798%)\n",
            "Epoch: [55][300/391]\tTime 0.057 (0.077)\tData 0.003 (0.004)\tLoss 0.0551 (0.1180)\tPrec 98.438% (95.775%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.211 (0.211)\tLoss 0.4078 (0.4078)\tPrec 89.062% (89.062%)\n",
            " * Prec 88.620% \n",
            "best acc: 88.660000\n",
            "Epoch: [56][0/391]\tTime 0.243 (0.243)\tData 0.143 (0.143)\tLoss 0.1081 (0.1081)\tPrec 96.094% (96.094%)\n",
            "Epoch: [56][100/391]\tTime 0.085 (0.069)\tData 0.004 (0.004)\tLoss 0.0875 (0.1180)\tPrec 97.656% (95.722%)\n",
            "Epoch: [56][200/391]\tTime 0.075 (0.074)\tData 0.004 (0.004)\tLoss 0.1004 (0.1197)\tPrec 95.312% (95.635%)\n",
            "Epoch: [56][300/391]\tTime 0.063 (0.072)\tData 0.002 (0.004)\tLoss 0.1505 (0.1193)\tPrec 93.750% (95.684%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.153 (0.153)\tLoss 0.2907 (0.2907)\tPrec 89.062% (89.062%)\n",
            " * Prec 88.200% \n",
            "best acc: 88.660000\n",
            "Epoch: [57][0/391]\tTime 0.252 (0.252)\tData 0.134 (0.134)\tLoss 0.1595 (0.1595)\tPrec 92.969% (92.969%)\n",
            "Epoch: [57][100/391]\tTime 0.107 (0.078)\tData 0.007 (0.005)\tLoss 0.0744 (0.1104)\tPrec 96.875% (96.063%)\n",
            "Epoch: [57][200/391]\tTime 0.075 (0.074)\tData 0.004 (0.004)\tLoss 0.1176 (0.1137)\tPrec 96.094% (95.950%)\n",
            "Epoch: [57][300/391]\tTime 0.067 (0.076)\tData 0.002 (0.004)\tLoss 0.1092 (0.1146)\tPrec 95.312% (95.899%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.159 (0.159)\tLoss 0.4103 (0.4103)\tPrec 88.281% (88.281%)\n",
            " * Prec 88.040% \n",
            "best acc: 88.660000\n",
            "Epoch: [58][0/391]\tTime 0.247 (0.247)\tData 0.141 (0.141)\tLoss 0.1016 (0.1016)\tPrec 94.531% (94.531%)\n",
            "Epoch: [58][100/391]\tTime 0.066 (0.081)\tData 0.003 (0.005)\tLoss 0.0689 (0.1090)\tPrec 96.875% (96.024%)\n",
            "Epoch: [58][200/391]\tTime 0.097 (0.075)\tData 0.007 (0.004)\tLoss 0.1477 (0.1127)\tPrec 94.531% (95.915%)\n",
            "Epoch: [58][300/391]\tTime 0.060 (0.076)\tData 0.003 (0.004)\tLoss 0.0930 (0.1140)\tPrec 96.875% (95.920%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.236 (0.236)\tLoss 0.2358 (0.2358)\tPrec 92.969% (92.969%)\n",
            " * Prec 88.290% \n",
            "best acc: 88.660000\n",
            "Epoch: [59][0/391]\tTime 0.235 (0.235)\tData 0.145 (0.145)\tLoss 0.1018 (0.1018)\tPrec 96.094% (96.094%)\n",
            "Epoch: [59][100/391]\tTime 0.064 (0.068)\tData 0.004 (0.004)\tLoss 0.1533 (0.1132)\tPrec 93.750% (96.055%)\n",
            "Epoch: [59][200/391]\tTime 0.077 (0.074)\tData 0.004 (0.004)\tLoss 0.1192 (0.1091)\tPrec 98.438% (96.203%)\n",
            "Epoch: [59][300/391]\tTime 0.089 (0.072)\tData 0.002 (0.004)\tLoss 0.1170 (0.1115)\tPrec 94.531% (96.050%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.161 (0.161)\tLoss 0.3149 (0.3149)\tPrec 88.281% (88.281%)\n",
            " * Prec 88.260% \n",
            "best acc: 88.660000\n",
            "Epoch: [60][0/391]\tTime 0.226 (0.226)\tData 0.140 (0.140)\tLoss 0.0355 (0.0355)\tPrec 99.219% (99.219%)\n",
            "Epoch: [60][100/391]\tTime 0.128 (0.079)\tData 0.011 (0.005)\tLoss 0.0796 (0.1126)\tPrec 97.656% (95.893%)\n",
            "Epoch: [60][200/391]\tTime 0.066 (0.075)\tData 0.002 (0.004)\tLoss 0.0702 (0.1140)\tPrec 96.875% (95.857%)\n",
            "Epoch: [60][300/391]\tTime 0.062 (0.077)\tData 0.003 (0.004)\tLoss 0.1086 (0.1138)\tPrec 96.875% (95.915%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.157 (0.157)\tLoss 0.3836 (0.3836)\tPrec 89.844% (89.844%)\n",
            " * Prec 88.550% \n",
            "best acc: 88.660000\n",
            "Epoch: [61][0/391]\tTime 0.328 (0.328)\tData 0.194 (0.194)\tLoss 0.0666 (0.0666)\tPrec 99.219% (99.219%)\n",
            "Epoch: [61][100/391]\tTime 0.063 (0.082)\tData 0.002 (0.006)\tLoss 0.0648 (0.1079)\tPrec 97.656% (96.086%)\n",
            "Epoch: [61][200/391]\tTime 0.104 (0.079)\tData 0.004 (0.004)\tLoss 0.0471 (0.1106)\tPrec 98.438% (96.032%)\n",
            "Epoch: [61][300/391]\tTime 0.061 (0.077)\tData 0.002 (0.004)\tLoss 0.0575 (0.1114)\tPrec 97.656% (95.964%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.236 (0.236)\tLoss 0.3494 (0.3494)\tPrec 91.406% (91.406%)\n",
            " * Prec 88.440% \n",
            "best acc: 88.660000\n",
            "Epoch: [62][0/391]\tTime 0.254 (0.254)\tData 0.163 (0.163)\tLoss 0.0499 (0.0499)\tPrec 98.438% (98.438%)\n",
            "Epoch: [62][100/391]\tTime 0.087 (0.070)\tData 0.003 (0.004)\tLoss 0.1367 (0.1091)\tPrec 95.312% (96.202%)\n",
            "Epoch: [62][200/391]\tTime 0.076 (0.075)\tData 0.004 (0.004)\tLoss 0.1321 (0.1128)\tPrec 93.750% (96.004%)\n",
            "Epoch: [62][300/391]\tTime 0.124 (0.073)\tData 0.004 (0.004)\tLoss 0.1582 (0.1141)\tPrec 95.312% (95.982%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.155 (0.155)\tLoss 0.3611 (0.3611)\tPrec 90.625% (90.625%)\n",
            " * Prec 87.820% \n",
            "best acc: 88.660000\n",
            "Epoch: [63][0/391]\tTime 0.232 (0.232)\tData 0.136 (0.136)\tLoss 0.1346 (0.1346)\tPrec 94.531% (94.531%)\n",
            "Epoch: [63][100/391]\tTime 0.120 (0.082)\tData 0.010 (0.005)\tLoss 0.0665 (0.1165)\tPrec 97.656% (95.846%)\n",
            "Epoch: [63][200/391]\tTime 0.068 (0.076)\tData 0.002 (0.004)\tLoss 0.1190 (0.1151)\tPrec 96.094% (95.876%)\n",
            "Epoch: [63][300/391]\tTime 0.063 (0.077)\tData 0.004 (0.004)\tLoss 0.1227 (0.1169)\tPrec 94.531% (95.847%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.158 (0.158)\tLoss 0.3575 (0.3575)\tPrec 90.625% (90.625%)\n",
            " * Prec 88.750% \n",
            "best acc: 88.750000\n",
            "Epoch: [64][0/391]\tTime 0.319 (0.319)\tData 0.167 (0.167)\tLoss 0.0908 (0.0908)\tPrec 96.875% (96.875%)\n",
            "Epoch: [64][100/391]\tTime 0.063 (0.076)\tData 0.002 (0.005)\tLoss 0.0501 (0.1092)\tPrec 97.656% (96.071%)\n",
            "Epoch: [64][200/391]\tTime 0.081 (0.078)\tData 0.004 (0.005)\tLoss 0.0485 (0.1079)\tPrec 97.656% (96.094%)\n",
            "Epoch: [64][300/391]\tTime 0.068 (0.075)\tData 0.004 (0.004)\tLoss 0.1230 (0.1088)\tPrec 96.875% (96.070%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.160 (0.160)\tLoss 0.3126 (0.3126)\tPrec 91.406% (91.406%)\n",
            " * Prec 88.150% \n",
            "best acc: 88.750000\n",
            "Epoch: [65][0/391]\tTime 0.255 (0.255)\tData 0.136 (0.136)\tLoss 0.1285 (0.1285)\tPrec 94.531% (94.531%)\n",
            "Epoch: [65][100/391]\tTime 0.092 (0.072)\tData 0.004 (0.004)\tLoss 0.2251 (0.1115)\tPrec 91.406% (96.086%)\n",
            "Epoch: [65][200/391]\tTime 0.081 (0.075)\tData 0.004 (0.004)\tLoss 0.1219 (0.1077)\tPrec 96.875% (96.206%)\n",
            "Epoch: [65][300/391]\tTime 0.108 (0.075)\tData 0.010 (0.004)\tLoss 0.1258 (0.1096)\tPrec 96.094% (96.138%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.150 (0.150)\tLoss 0.4700 (0.4700)\tPrec 85.938% (85.938%)\n",
            " * Prec 88.700% \n",
            "best acc: 88.750000\n",
            "Epoch: [66][0/391]\tTime 0.265 (0.265)\tData 0.142 (0.142)\tLoss 0.0516 (0.0516)\tPrec 97.656% (97.656%)\n",
            "Epoch: [66][100/391]\tTime 0.084 (0.082)\tData 0.002 (0.005)\tLoss 0.0960 (0.1057)\tPrec 96.875% (96.241%)\n",
            "Epoch: [66][200/391]\tTime 0.075 (0.074)\tData 0.002 (0.004)\tLoss 0.1477 (0.1078)\tPrec 95.312% (96.148%)\n",
            "Epoch: [66][300/391]\tTime 0.061 (0.075)\tData 0.002 (0.004)\tLoss 0.0980 (0.1089)\tPrec 96.875% (96.109%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.153 (0.153)\tLoss 0.4613 (0.4613)\tPrec 87.500% (87.500%)\n",
            " * Prec 88.420% \n",
            "best acc: 88.750000\n",
            "Epoch: [67][0/391]\tTime 0.369 (0.369)\tData 0.251 (0.251)\tLoss 0.1716 (0.1716)\tPrec 93.750% (93.750%)\n",
            "Epoch: [67][100/391]\tTime 0.065 (0.075)\tData 0.002 (0.006)\tLoss 0.1067 (0.1062)\tPrec 96.094% (96.094%)\n",
            "Epoch: [67][200/391]\tTime 0.090 (0.077)\tData 0.008 (0.005)\tLoss 0.1010 (0.1037)\tPrec 97.656% (96.187%)\n",
            "Epoch: [67][300/391]\tTime 0.059 (0.073)\tData 0.002 (0.004)\tLoss 0.1154 (0.1081)\tPrec 96.094% (96.047%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.156 (0.156)\tLoss 0.3894 (0.3894)\tPrec 90.625% (90.625%)\n",
            " * Prec 88.460% \n",
            "best acc: 88.750000\n",
            "Epoch: [68][0/391]\tTime 0.228 (0.228)\tData 0.135 (0.135)\tLoss 0.1829 (0.1829)\tPrec 92.969% (92.969%)\n",
            "Epoch: [68][100/391]\tTime 0.059 (0.068)\tData 0.002 (0.004)\tLoss 0.0937 (0.1060)\tPrec 96.875% (96.117%)\n",
            "Epoch: [68][200/391]\tTime 0.075 (0.073)\tData 0.003 (0.004)\tLoss 0.0914 (0.1036)\tPrec 96.875% (96.315%)\n",
            "Epoch: [68][300/391]\tTime 0.071 (0.070)\tData 0.002 (0.003)\tLoss 0.0988 (0.1045)\tPrec 96.094% (96.242%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.157 (0.157)\tLoss 0.4562 (0.4562)\tPrec 87.500% (87.500%)\n",
            " * Prec 88.260% \n",
            "best acc: 88.750000\n",
            "Epoch: [69][0/391]\tTime 0.251 (0.251)\tData 0.131 (0.131)\tLoss 0.1032 (0.1032)\tPrec 96.094% (96.094%)\n",
            "Epoch: [69][100/391]\tTime 0.066 (0.079)\tData 0.004 (0.005)\tLoss 0.0701 (0.0999)\tPrec 97.656% (96.388%)\n",
            "Epoch: [69][200/391]\tTime 0.077 (0.073)\tData 0.009 (0.004)\tLoss 0.1000 (0.1045)\tPrec 96.094% (96.265%)\n",
            "Epoch: [69][300/391]\tTime 0.057 (0.074)\tData 0.002 (0.003)\tLoss 0.1166 (0.1038)\tPrec 93.750% (96.288%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.158 (0.158)\tLoss 0.4278 (0.4278)\tPrec 89.844% (89.844%)\n",
            " * Prec 88.050% \n",
            "best acc: 88.750000\n",
            "Epoch: [70][0/391]\tTime 0.338 (0.338)\tData 0.187 (0.187)\tLoss 0.0330 (0.0330)\tPrec 99.219% (99.219%)\n",
            "Epoch: [70][100/391]\tTime 0.063 (0.079)\tData 0.002 (0.005)\tLoss 0.1055 (0.1011)\tPrec 96.094% (96.334%)\n",
            "Epoch: [70][200/391]\tTime 0.082 (0.075)\tData 0.003 (0.004)\tLoss 0.0804 (0.1022)\tPrec 96.875% (96.269%)\n",
            "Epoch: [70][300/391]\tTime 0.092 (0.074)\tData 0.003 (0.004)\tLoss 0.1191 (0.1046)\tPrec 97.656% (96.278%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.228 (0.228)\tLoss 0.3166 (0.3166)\tPrec 89.844% (89.844%)\n",
            " * Prec 88.590% \n",
            "best acc: 88.750000\n",
            "Epoch: [71][0/391]\tTime 0.234 (0.234)\tData 0.139 (0.139)\tLoss 0.1570 (0.1570)\tPrec 94.531% (94.531%)\n",
            "Epoch: [71][100/391]\tTime 0.079 (0.068)\tData 0.004 (0.004)\tLoss 0.0340 (0.0968)\tPrec 99.219% (96.550%)\n",
            "Epoch: [71][200/391]\tTime 0.092 (0.073)\tData 0.002 (0.004)\tLoss 0.0733 (0.1042)\tPrec 97.656% (96.199%)\n",
            "Epoch: [71][300/391]\tTime 0.075 (0.070)\tData 0.004 (0.003)\tLoss 0.0653 (0.1042)\tPrec 97.656% (96.229%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.154 (0.154)\tLoss 0.5020 (0.5020)\tPrec 89.844% (89.844%)\n",
            " * Prec 88.280% \n",
            "best acc: 88.750000\n",
            "Epoch: [72][0/391]\tTime 0.259 (0.259)\tData 0.141 (0.141)\tLoss 0.0577 (0.0577)\tPrec 97.656% (97.656%)\n",
            "Epoch: [72][100/391]\tTime 0.112 (0.079)\tData 0.005 (0.005)\tLoss 0.1484 (0.1033)\tPrec 94.531% (96.287%)\n",
            "Epoch: [72][200/391]\tTime 0.057 (0.073)\tData 0.002 (0.004)\tLoss 0.1448 (0.1018)\tPrec 94.531% (96.354%)\n",
            "Epoch: [72][300/391]\tTime 0.059 (0.074)\tData 0.002 (0.004)\tLoss 0.0902 (0.1026)\tPrec 97.656% (96.338%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.165 (0.165)\tLoss 0.3000 (0.3000)\tPrec 91.406% (91.406%)\n",
            " * Prec 88.370% \n",
            "best acc: 88.750000\n",
            "Epoch: [73][0/391]\tTime 0.267 (0.267)\tData 0.145 (0.145)\tLoss 0.0961 (0.0961)\tPrec 96.094% (96.094%)\n",
            "Epoch: [73][100/391]\tTime 0.082 (0.082)\tData 0.002 (0.005)\tLoss 0.0871 (0.1052)\tPrec 96.094% (96.279%)\n",
            "Epoch: [73][200/391]\tTime 0.130 (0.077)\tData 0.004 (0.004)\tLoss 0.0530 (0.1067)\tPrec 99.219% (96.249%)\n",
            "Epoch: [73][300/391]\tTime 0.077 (0.077)\tData 0.004 (0.004)\tLoss 0.1350 (0.1042)\tPrec 92.188% (96.322%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.254 (0.254)\tLoss 0.3247 (0.3247)\tPrec 91.406% (91.406%)\n",
            " * Prec 88.780% \n",
            "best acc: 88.780000\n",
            "Epoch: [74][0/391]\tTime 0.228 (0.228)\tData 0.158 (0.158)\tLoss 0.1120 (0.1120)\tPrec 97.656% (97.656%)\n",
            "Epoch: [74][100/391]\tTime 0.062 (0.070)\tData 0.002 (0.004)\tLoss 0.0588 (0.0954)\tPrec 96.875% (96.604%)\n",
            "Epoch: [74][200/391]\tTime 0.061 (0.075)\tData 0.002 (0.004)\tLoss 0.1131 (0.0944)\tPrec 94.531% (96.568%)\n",
            "Epoch: [74][300/391]\tTime 0.082 (0.073)\tData 0.002 (0.004)\tLoss 0.0986 (0.0989)\tPrec 96.094% (96.439%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.156 (0.156)\tLoss 0.4013 (0.4013)\tPrec 89.062% (89.062%)\n",
            " * Prec 88.550% \n",
            "best acc: 88.780000\n",
            "Epoch: [75][0/391]\tTime 0.232 (0.232)\tData 0.137 (0.137)\tLoss 0.0517 (0.0517)\tPrec 99.219% (99.219%)\n",
            "Epoch: [75][100/391]\tTime 0.094 (0.082)\tData 0.003 (0.005)\tLoss 0.0745 (0.0945)\tPrec 99.219% (96.496%)\n",
            "Epoch: [75][200/391]\tTime 0.069 (0.075)\tData 0.002 (0.004)\tLoss 0.1099 (0.0981)\tPrec 96.875% (96.444%)\n",
            "Epoch: [75][300/391]\tTime 0.096 (0.077)\tData 0.004 (0.004)\tLoss 0.0801 (0.0994)\tPrec 96.875% (96.410%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.156 (0.156)\tLoss 0.3964 (0.3964)\tPrec 90.625% (90.625%)\n",
            " * Prec 88.770% \n",
            "best acc: 88.780000\n",
            "Epoch: [76][0/391]\tTime 0.328 (0.328)\tData 0.182 (0.182)\tLoss 0.0584 (0.0584)\tPrec 96.875% (96.875%)\n",
            "Epoch: [76][100/391]\tTime 0.078 (0.082)\tData 0.004 (0.005)\tLoss 0.1901 (0.1000)\tPrec 94.531% (96.403%)\n",
            "Epoch: [76][200/391]\tTime 0.111 (0.078)\tData 0.007 (0.004)\tLoss 0.0843 (0.0992)\tPrec 96.875% (96.424%)\n",
            "Epoch: [76][300/391]\tTime 0.063 (0.077)\tData 0.004 (0.004)\tLoss 0.0538 (0.1018)\tPrec 97.656% (96.307%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.180 (0.180)\tLoss 0.4105 (0.4105)\tPrec 89.062% (89.062%)\n",
            " * Prec 88.320% \n",
            "best acc: 88.780000\n",
            "Epoch: [77][0/391]\tTime 0.236 (0.236)\tData 0.135 (0.135)\tLoss 0.0710 (0.0710)\tPrec 98.438% (98.438%)\n",
            "Epoch: [77][100/391]\tTime 0.060 (0.069)\tData 0.002 (0.004)\tLoss 0.0708 (0.0940)\tPrec 96.875% (96.689%)\n",
            "Epoch: [77][200/391]\tTime 0.061 (0.075)\tData 0.002 (0.004)\tLoss 0.0734 (0.0969)\tPrec 97.656% (96.529%)\n",
            "Epoch: [77][300/391]\tTime 0.094 (0.073)\tData 0.002 (0.004)\tLoss 0.1306 (0.1003)\tPrec 96.094% (96.444%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.155 (0.155)\tLoss 0.3648 (0.3648)\tPrec 90.625% (90.625%)\n",
            " * Prec 88.390% \n",
            "best acc: 88.780000\n",
            "Epoch: [78][0/391]\tTime 0.236 (0.236)\tData 0.143 (0.143)\tLoss 0.1363 (0.1363)\tPrec 94.531% (94.531%)\n",
            "Epoch: [78][100/391]\tTime 0.099 (0.081)\tData 0.004 (0.005)\tLoss 0.1243 (0.1048)\tPrec 95.312% (96.202%)\n",
            "Epoch: [78][200/391]\tTime 0.072 (0.075)\tData 0.003 (0.004)\tLoss 0.1502 (0.1014)\tPrec 96.875% (96.424%)\n",
            "Epoch: [78][300/391]\tTime 0.062 (0.077)\tData 0.004 (0.004)\tLoss 0.0821 (0.1000)\tPrec 96.094% (96.442%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.154 (0.154)\tLoss 0.2789 (0.2789)\tPrec 89.844% (89.844%)\n",
            " * Prec 88.260% \n",
            "best acc: 88.780000\n",
            "Epoch: [79][0/391]\tTime 0.387 (0.387)\tData 0.226 (0.226)\tLoss 0.1254 (0.1254)\tPrec 96.094% (96.094%)\n",
            "Epoch: [79][100/391]\tTime 0.060 (0.075)\tData 0.002 (0.005)\tLoss 0.0685 (0.1028)\tPrec 96.875% (96.411%)\n",
            "Epoch: [79][200/391]\tTime 0.109 (0.075)\tData 0.003 (0.004)\tLoss 0.0998 (0.1002)\tPrec 95.312% (96.459%)\n",
            "Epoch: [79][300/391]\tTime 0.058 (0.073)\tData 0.002 (0.004)\tLoss 0.0942 (0.0999)\tPrec 96.875% (96.449%)\n",
            "Validation starts\n",
            "Test: [0/79]\tTime 0.177 (0.177)\tLoss 0.3097 (0.3097)\tPrec 90.625% (90.625%)\n",
            " * Prec 88.330% \n",
            "best acc: 88.780000\n"
          ]
        }
      ],
      "source": [
        "# This cell won't be given, but students will complete the training\n",
        "\n",
        "lr = 6e-3\n",
        "weight_decay = 1e-5\n",
        "epochs = 80\n",
        "best_prec = 0\n",
        "\n",
        "\n",
        "if not os.path.exists('result'):\n",
        "    os.makedirs('result')\n",
        "fdir = '/content/drive/MyDrive/result/'+str(model_name)\n",
        "if not os.path.exists(fdir):\n",
        "    os.makedirs(fdir)\n",
        "else:\n",
        "  checkpoint = torch.load(fdir+ '/model_best.pth.tar')\n",
        "  model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "#model = nn.DataParallel(model).cuda()\n",
        "model.cuda()\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
        "#cudnn.benchmark = True\n",
        "\n",
        "\n",
        "for epoch in range(0, epochs):\n",
        "    adjust_learning_rate(optimizer, epoch)\n",
        "\n",
        "    train(trainloader, model, criterion, optimizer, epoch)\n",
        "\n",
        "    # evaluate on test set\n",
        "    print(\"Validation starts\")\n",
        "    prec = validate(testloader, model, criterion)\n",
        "\n",
        "    # remember best precision and save checkpoint\n",
        "    is_best = prec > best_prec\n",
        "    best_prec = max(prec,best_prec)\n",
        "    print('best acc: {:1f}'.format(best_prec))\n",
        "    save_checkpoint({\n",
        "        'epoch': epoch + 1,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'best_prec': best_prec,\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "    }, is_best, fdir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "decreased-harris",
      "metadata": {
        "id": "decreased-harris"
      },
      "outputs": [],
      "source": [
        "# HW\n",
        "\n",
        "#  1. Train with 4 bits for both weight and activation to achieve >90% accuracy\n",
        "#  2. Find x_int and w_int for the 2nd convolution layer\n",
        "#  3. Check the recovered psum has similar value to the un-quantized original psum\n",
        "#     (such as example 1 in W3S2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "entertaining-queensland",
      "metadata": {
        "id": "entertaining-queensland"
      },
      "outputs": [],
      "source": [
        "PATH = \"result/VGG16_quant/model_best.pth.tar\"\n",
        "checkpoint = torch.load(PATH)\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "model.cuda()\n",
        "model.eval()\n",
        "\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, target in testloader:\n",
        "        data, target = data.to(device), target.to(device) # loading to GPU\n",
        "        output = model(data)\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "test_loss /= len(testloader.dataset)\n",
        "\n",
        "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        correct, len(testloader.dataset),\n",
        "        100. * correct / len(testloader.dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ceramic-nigeria",
      "metadata": {
        "id": "ceramic-nigeria"
      },
      "outputs": [],
      "source": [
        "#send an input and grap the value by using prehook like HW3\n",
        "\n",
        "class SaveOutput:\n",
        "    def __init__(self):\n",
        "        self.outputs = []\n",
        "    def __call__(self, module, module_in):\n",
        "        self.outputs.append(module_in)\n",
        "    def clear(self):\n",
        "        self.outputs = []\n",
        "\n",
        "\n",
        "save_output = SaveOutput()\n",
        "for layer in model.modules():\n",
        "     if isinstance(layer, nn.Conv2d): # hook input laters of all conv layers\n",
        "        layer.register_forward_pre_hook(save_output)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def act_quantization(b):\n",
        "\n",
        "    def uniform_quant(x, b=4):\n",
        "        xdiv = x.mul(2 ** b - 1)\n",
        "        xhard = xdiv.round().div(2 ** b - 1)\n",
        "        return xhard\n",
        "\n",
        "    class _uq(torch.autograd.Function):\n",
        "        @staticmethod\n",
        "        def forward(ctx, input, alpha):\n",
        "            input=input.div(alpha)\n",
        "            input_c = input.clamp(max=1)  # Mingu edited for Alexnet\n",
        "            input_q = uniform_quant(input_c, b)\n",
        "            ctx.save_for_backward(input, input_q)\n",
        "            input_q = input_q.mul(alpha)\n",
        "            return input_q\n",
        "\n",
        "        @staticmethod\n",
        "        def backward(ctx, grad_output):\n",
        "            grad_input = grad_output.clone()\n",
        "            input, input_q = ctx.saved_tensors\n",
        "            i = (input > 1.).float()\n",
        "            #grad_alpha = (grad_output * (i + (input_q - input) * (1 - i))).sum()\n",
        "            grad_alpha = (grad_output * (i + (0.0)*(1-i))).sum()\n",
        "            grad_input = grad_input*(1-i)\n",
        "            return grad_input, grad_alpha\n",
        "\n",
        "    return _uq().apply"
      ],
      "metadata": {
        "id": "yCttQK4G-5UM"
      },
      "id": "yCttQK4G-5UM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "spoken-worst",
      "metadata": {
        "id": "spoken-worst"
      },
      "outputs": [],
      "source": [
        "w_bit = 4\n",
        "weight_q = model.features[3].weight_q     # quantized value is stored during the training\n",
        "w_alpha = model.show_params()             # alpha is defined in your model already. bring it out here\n",
        "w_delta = w_alpha / (2 ** (w_bit-1) - 1)  # delta can be calculated by using alpha and w_bit\n",
        "weight_int = weight_q * w_delta           # w_int can be calculated by weight_q and w_delta\n",
        "print(weight_int)                         # you should see clean integer numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "interior-oxygen",
      "metadata": {
        "id": "interior-oxygen"
      },
      "outputs": [],
      "source": [
        "x_bit = 4\n",
        "x = model.features[1]                  # input of the 2nd conv layer\n",
        "x_alpha  = model.show_params()\n",
        "x_delta = x_alpha / (2 ** x_bit - 1)\n",
        "\n",
        "act_quant_fn = act_quantization(x_bit) # define the quantization function\n",
        "x_q = act_quant_fn(x, x_alpha)         # create the quantized value for x\n",
        "\n",
        "x_int = x_q / x_delta\n",
        "print(x_int)                           # you should see clean integer numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ranging-porter",
      "metadata": {
        "id": "ranging-porter"
      },
      "outputs": [],
      "source": [
        "conv_int = torch.nn.Conv2d(in_channels = 64, out_channels=64, kernel_size = 3, bias = False)\n",
        "conv_int.weight = torch.nn.parameter.Parameter(weight_int)\n",
        "\n",
        "output_int =  conv_int * x_int    # output_int can be calculated with conv_int and x_int\n",
        "output_recovered = output_int / w_delta / x_delta  # recover with x_delta and w_delta\n",
        "print(output_recovered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "designed-auction",
      "metadata": {
        "id": "designed-auction"
      },
      "outputs": [],
      "source": [
        "#### input floating number / weight quantized version\n",
        "\n",
        "conv_ref = torch.nn.Conv2d(in_channels = 64, out_channels=64, kernel_size = 3, bias = False)\n",
        "conv_ref.weight = model.features[3].weight_q\n",
        "\n",
        "output_ref = conv_ref(x)\n",
        "print(output_ref)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "157dffd8",
      "metadata": {
        "id": "157dffd8"
      },
      "outputs": [],
      "source": [
        "difference = abs( output_ref - output_recovered )\n",
        "print(difference.mean())  ## It should be small, e.g.,2.3 in my trainned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sorted-niger",
      "metadata": {
        "id": "sorted-niger"
      },
      "outputs": [],
      "source": [
        "#### input floating number / weight floating number version\n",
        "\n",
        "conv_ref = torch.nn.Conv2d(in_channels = 64, out_channels=64, kernel_size = 3, bias = False)\n",
        "weight = model.features[3].weight\n",
        "mean = weight.data.mean()\n",
        "std = weight.data.std()\n",
        "conv_ref.weight = torch.nn.parameter.Parameter(weight.add(-mean).div(std))\n",
        "\n",
        "output_ref = conv_ref(x)\n",
        "print(output_ref)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "significant-whole",
      "metadata": {
        "id": "significant-whole"
      },
      "outputs": [],
      "source": [
        "difference = abs( output_ref - output_recovered )\n",
        "print(difference.mean())  ## It should be small, e.g.,2.3 in my trainned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "corresponding-significance",
      "metadata": {
        "id": "corresponding-significance"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "exposed-witch",
      "metadata": {
        "id": "exposed-witch"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "entitled-barbados",
      "metadata": {
        "id": "entitled-barbados"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "minimal-serbia",
      "metadata": {
        "id": "minimal-serbia"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}